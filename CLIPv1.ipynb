{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "be515a7e0f884c74aca019c3f0310627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e3cc3f09a4a4aa9a15cd969ae618475",
              "IPY_MODEL_20650c157b2f40a28c7a79b324656838",
              "IPY_MODEL_dc3343f4aa09460ead29ee934bf34e3f"
            ],
            "layout": "IPY_MODEL_f71e9d89831144949784c0bd6ec2b8ce"
          }
        },
        "9e3cc3f09a4a4aa9a15cd969ae618475": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_406fc5279035493e9ffcb93a0f5f3b48",
            "placeholder": "​",
            "style": "IPY_MODEL_55bb7acb52bc4c85a7138ee515c51fe7",
            "value": "100%"
          }
        },
        "20650c157b2f40a28c7a79b324656838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c7dd37d8014469698c2f50f1c73a5cf",
            "max": 169001437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46b0340271e44c26b6effc03924c7d06",
            "value": 169001437
          }
        },
        "dc3343f4aa09460ead29ee934bf34e3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4a9d187638543a2b4c233bd00a10a30",
            "placeholder": "​",
            "style": "IPY_MODEL_4493908946584e7f8697b899e2a894de",
            "value": " 169001437/169001437 [00:01&lt;00:00, 104760280.49it/s]"
          }
        },
        "f71e9d89831144949784c0bd6ec2b8ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "406fc5279035493e9ffcb93a0f5f3b48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55bb7acb52bc4c85a7138ee515c51fe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c7dd37d8014469698c2f50f1c73a5cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46b0340271e44c26b6effc03924c7d06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4a9d187638543a2b4c233bd00a10a30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4493908946584e7f8697b899e2a894de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## TEST"
      ],
      "metadata": {
        "id": "gUYN9VMeeJ_s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwfLRiYgVRvS",
        "outputId": "0a040282-edfc-46c2-ac8d-2e9868ee12db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-bhn4skmj\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-bhn4skmj\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.13.1+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369409 sha256=4f3ad7aaef02e43d43d3a50e657035126a7dab183dd51a60b1311c268974b394\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zf66gfad/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd7bUhFnVnAs",
        "outputId": "819c4edf-38b1-44f2-edd2-4078c87e8f9f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:01<00:00, 245MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "aAKS9YSzWYrN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e2a0a91f-739e-4792-ad67-0b90f81b0e6f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. map\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "image = preprocess(Image.open(\"/content/drive/MyDrive/Pictures/map.JPG\")).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "7Tj8x71TWk5p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c877414-10ce-4b64-e280-bcb18fcb9883"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = clip.tokenize(['chick','map','picture','white','a white map','a colorful map']).to(device)\n",
        "with torch.no_grad():\n",
        "  image_features = model.encode_image(image)\n",
        "  text_features = model.encode_text(text)\n",
        "\n",
        "  logits_per_image, logits_per_text = model(image,text)\n",
        "  probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "print(\"Label probs:\", probs)"
      ],
      "metadata": {
        "id": "V5hGYq6na6EW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5136f05d-84c8-437d-f4c1-d166ba972aa8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label probs: [[5.364e-05 1.375e-02 7.445e-05 9.412e-05 1.869e-01 7.993e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. dog and person\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "image = preprocess(Image.open(\"/content/drive/MyDrive/Pictures/tx1.jpeg\")).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24Y9jTYGI4gB",
        "outputId": "b782c496-549a-4c37-ba81-ea1458626e58"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = clip.tokenize(['person','dog','cartoon','anime','white','blue','a person with a dog','a person laughing with a dog']).to(device) #anime\n",
        "text2 = clip.tokenize(['a person with a dog','a person laughing with a dog','a person in white with a dog','a person in blue with a dog']).to(device) #blue..\n",
        "# cannot detect laughing.  Fine grained?\n",
        "with torch.no_grad():\n",
        "  image_features = model.encode_image(image)\n",
        "  text_features = model.encode_text(text)\n",
        "\n",
        "  logits_per_image, logits_per_text = model(image,text)\n",
        "  probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "with torch.no_grad():\n",
        "  image_features = model.encode_image(image)\n",
        "  text_features = model.encode_text(text2)\n",
        "\n",
        "  logits_per_image, logits_per_text = model(image,text2)\n",
        "  probs2 = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "print(\"Label probs:\", probs)\n",
        "print(\"Label probs2:\", probs2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58e80b4a-64a0-40af-8f44-8020162fe1c7",
        "id": "JRew4cLrI6yI"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label probs: [[0.02971  0.008125 0.00576  0.5435   0.02463  0.1353   0.2488   0.004417]]\n",
            "Label probs2: [[0.2433  0.00432 0.1595  0.593  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. laughing\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "image = preprocess(Image.open(\"/content/drive/MyDrive/Pictures/laughing.jpg\")).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aHE7cVEScvE",
        "outputId": "3f16469c-b01f-4bba-d757-ff9fb6312982"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = clip.tokenize(['person','dog','chick','documents','picture','white','blue','a person with a dog','a person laughing with a dog']).to(device)\n",
        "with torch.no_grad():\n",
        "  image_features = model.encode_image(image)\n",
        "  text_features = model.encode_text(text)\n",
        "\n",
        "  logits_per_image, logits_per_text = model(image,text)\n",
        "  probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "print(\"Label probs:\", probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtJSRcRrSetX",
        "outputId": "8af07fdd-23f3-4883-9b4e-eba4e4579fea"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label probs: [[5.186e-06 1.534e-04 3.576e-07 5.960e-08 2.325e-06 2.146e-06 1.788e-07\n",
            "  8.168e-04 9.990e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. buaa \n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "image = preprocess(Image.open(\"/content/drive/MyDrive/Pictures/buaa.png\")).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946ae9d5-1101-4b00-d1ab-42fdcf9c58f1",
        "id": "vuWRdoh9KZ6N"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = clip.tokenize(['blue','medal','emblem','symbol','beihang','beihang university','emblem of beihang university']).to(device)\n",
        "with torch.no_grad():\n",
        "  image_features = model.encode_image(image)\n",
        "  text_features = model.encode_text(text)\n",
        "\n",
        "  logits_per_image, logits_per_text = model(image,text)\n",
        "  probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "print(\"Label probs:\", probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5407b344-de84-4f8c-aafd-987be766bd5b",
        "id": "MUydq1NwKcBc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label probs: [[1.192e-07 5.960e-08 2.944e-05 6.527e-05 1.087e-03 4.080e-02 9.580e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. NLP\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "image = preprocess(Image.open(\"/content/drive/MyDrive/Pictures/NLP.jpg\")).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1f0b2e5-bbe4-4e06-c1ec-5bbaf13248c2",
        "id": "rzBBlTapL7x8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = clip.tokenize(['nlp','document','picture of nlp','excel','white','words','screen shot']).to(device)\n",
        "with torch.no_grad():\n",
        "  image_features = model.encode_image(image)\n",
        "  text_features = model.encode_text(text)\n",
        "\n",
        "  logits_per_image, logits_per_text = model(image,text)\n",
        "  probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "print(\"Label probs:\", probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmQ6g8JQL9Xn",
        "outputId": "fb344a0e-8a0b-4261-ed11-3aae698257af"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label probs: [[0.427    0.0518   0.4993   0.01438  0.00204  0.002205 0.003416]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Green\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "image = preprocess(Image.open(\"/content/drive/MyDrive/Pictures/green.jpg\")).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de994207-672d-4487-e4f9-e652a3c6f7e6",
        "id": "LPXtK0-GNQW-"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = clip.tokenize(['architecture','sky','sky and architecture','white house']).to(device)\n",
        "with torch.no_grad():\n",
        "  image_features = model.encode_image(image)\n",
        "  text_features = model.encode_text(text)\n",
        "\n",
        "  logits_per_image, logits_per_text = model(image,text)\n",
        "  probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "print(\"Label probs:\", probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2muDfKV-NH3i",
        "outputId": "7122e2c2-229f-4f87-bc9f-cdaa4563d6b1"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label probs: [[0.1416   0.3674   0.4868   0.004147]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test finished. \\\\"
      ],
      "metadata": {
        "id": "rCYYXeof4FSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR100"
      ],
      "metadata": {
        "id": "c-Lm35v1_zpO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44af283f-28c4-49c0-adeb-cdd48c442222",
        "id": "rsvv0kFXiknA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-1o08krhk\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-1o08krhk\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.13.1+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2022.6.15)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369409 sha256=06b4d46b1a4b17ba7445724246880b9d784bee854097c96236b355fffc44d470\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ovun5grx/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Download the dataset\n",
        "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
        "\n",
        "# Prepare the inputs\n",
        "image, class_id = cifar100[3637]\n",
        "image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
        "\n",
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input)\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "\n",
        "# Pick the top 5 most similar labels for the image\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "values, indices = similarity[0].topk(5)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nTop predictions:\\n\")\n",
        "for value, index in zip(values, indices):\n",
        "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247,
          "referenced_widgets": [
            "be515a7e0f884c74aca019c3f0310627",
            "9e3cc3f09a4a4aa9a15cd969ae618475",
            "20650c157b2f40a28c7a79b324656838",
            "dc3343f4aa09460ead29ee934bf34e3f",
            "f71e9d89831144949784c0bd6ec2b8ce",
            "406fc5279035493e9ffcb93a0f5f3b48",
            "55bb7acb52bc4c85a7138ee515c51fe7",
            "5c7dd37d8014469698c2f50f1c73a5cf",
            "46b0340271e44c26b6effc03924c7d06",
            "b4a9d187638543a2b4c233bd00a10a30",
            "4493908946584e7f8697b899e2a894de"
          ]
        },
        "id": "1l8lYRqGR7mw",
        "outputId": "7f8e3867-1232-4116-9097-1f8f089554db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:01<00:00, 185MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /root/.cache/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/169001437 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be515a7e0f884c74aca019c3f0310627"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.cache/cifar-100-python.tar.gz to /root/.cache\n",
            "\n",
            "Top predictions:\n",
            "\n",
            "           snake: 65.14%\n",
            "          turtle: 12.43%\n",
            "    sweet_pepper: 3.91%\n",
            "          lizard: 1.85%\n",
            "       crocodile: 1.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices\n",
        "print(indices)\n",
        "val = []\n",
        "idx = []\n",
        "for value, index in zip(values, indices):\n",
        "    val.append(value)\n",
        "    idx.append(index)\n",
        "print(val)\n",
        "print(idx)\n",
        "print(idx[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBcznD0MJbMW",
        "outputId": "49d932cb-cdf1-438a-b320-b4534b90512b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([78, 93, 83, 44, 27], device='cuda:0')\n",
            "[tensor(0.6514, device='cuda:0', dtype=torch.float16), tensor(0.1243, device='cuda:0', dtype=torch.float16), tensor(0.0391, device='cuda:0', dtype=torch.float16), tensor(0.0185, device='cuda:0', dtype=torch.float16), tensor(0.0171, device='cuda:0', dtype=torch.float16)]\n",
            "[tensor(78, device='cuda:0'), tensor(93, device='cuda:0'), tensor(83, device='cuda:0'), tensor(44, device='cuda:0'), tensor(27, device='cuda:0')]\n",
            "tensor(78, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(cifar100)"
      ],
      "metadata": {
        "id": "PaqnkXVrnA38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar100.class_to_idx"
      ],
      "metadata": {
        "id": "zFFXRuMSnFL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Syk0UDEJUvT",
        "outputId": "b414f034-8abe-49b2-be65-61e0eba52ea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([58], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUsnKEPRFQu7",
        "outputId": "7dd16ed8-8d81-4b80-a04c-0f11ca826ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xkx2W-ghFT6q",
        "outputId": "0d45c357-cec3-484c-f605-e28523be0b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(58, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_id==index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka9i5m3Ue7Ux",
        "outputId": "5808012e-4da8-49c7-e5de-2b4cc9449985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR final"
      ],
      "metadata": {
        "id": "8mMKsFgGd8_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Download the dataset\n",
        "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
        "\n",
        "correct = 0\n",
        "correct5 = 0\n",
        "count = 0\n",
        "val = []\n",
        "idx = []\n",
        "for image, class_id in cifar100:\n",
        "  count = count + 1\n",
        "  if(count%500==0):\n",
        "    print(f\"No.{count} finished\")\n",
        "  image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "  text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
        "\n",
        "# Calculate features\n",
        "  with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input)\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "\n",
        "# Pick the top 5 most similar labels for the image\n",
        "  image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "  text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "  similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "  values, indices = similarity[0].topk(5)\n",
        "\n",
        "  for value, index in zip(values, indices):\n",
        "    val.append(value)\n",
        "    idx.append(index)\n",
        "  if class_id==idx[0]:\n",
        "    correct = correct + 1\n",
        "  if class_id in idx:\n",
        "    correct5 = correct5 + 1\n",
        "  val = []\n",
        "  idx = []\n",
        "\n",
        "print(f\"CIFAR100： correct: {correct}, precision: {correct/10000}\")\n",
        "print(f\"CIFAR100： correct5: {correct5}, precision5: {correct5/10000}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMHxOYSgfCPn",
        "outputId": "d33b6cd9-1637-4573-d5be-6fd1b575ded3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No.500 finished\n",
            "No.1000 finished\n",
            "No.1500 finished\n",
            "No.2000 finished\n",
            "No.2500 finished\n",
            "No.3000 finished\n",
            "No.3500 finished\n",
            "No.4000 finished\n",
            "No.4500 finished\n",
            "No.5000 finished\n",
            "No.5500 finished\n",
            "No.6000 finished\n",
            "No.6500 finished\n",
            "No.7000 finished\n",
            "No.7500 finished\n",
            "No.8000 finished\n",
            "No.8500 finished\n",
            "No.9000 finished\n",
            "No.9500 finished\n",
            "No.10000 finished\n",
            "CIFAR100： correct: 6168, precision: 0.6168\n",
            "CIFAR100： correct5: 8672, precision5: 0.8672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Download the dataset\n",
        "cifar10 = CIFAR10(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
        "\n",
        "\n",
        "correct = 0\n",
        "correct5 = 0\n",
        "count = 0\n",
        "val = []\n",
        "idx = []\n",
        "for image, class_id in cifar10:\n",
        "  count = count + 1\n",
        "  if(count%500==0):\n",
        "    print(f\"No.{count} finished\")\n",
        "  image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "  text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar10.classes]).to(device)\n",
        "\n",
        "# Calculate features\n",
        "  with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input)\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "\n",
        "# Pick the top 5 most similar labels for the image\n",
        "  image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "  text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "  similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "  values, indices = similarity[0].topk(5)\n",
        "\n",
        "  for value, index in zip(values, indices):\n",
        "    val.append(value)\n",
        "    idx.append(index)\n",
        "  if class_id==idx[0]:\n",
        "    correct = correct + 1\n",
        "  if class_id in idx:\n",
        "    correct5 = correct5 + 1\n",
        "  val = []\n",
        "  idx = []\n",
        "\n",
        "print(f\"CIFAR10： correct: {correct}, precision: {correct/10000}\")\n",
        "print(f\"CIFAR10： correct5: {correct5}, precision5: {correct5/10000}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx9xE1uTpKLH",
        "outputId": "47c99c52-8700-4fab-b3fb-7ec29543e440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "No.500 finished\n",
            "No.1000 finished\n",
            "No.1500 finished\n",
            "No.2000 finished\n",
            "No.2500 finished\n",
            "No.3000 finished\n",
            "No.3500 finished\n",
            "No.4000 finished\n",
            "No.4500 finished\n",
            "No.5000 finished\n",
            "No.5500 finished\n",
            "No.6000 finished\n",
            "No.6500 finished\n",
            "No.7000 finished\n",
            "No.7500 finished\n",
            "No.8000 finished\n",
            "No.8500 finished\n",
            "No.9000 finished\n",
            "No.9500 finished\n",
            "No.10000 finished\n",
            "CIFAR10： correct: 8876, precision: 0.8876\n",
            "CIFAR10： correct5: 9938, precision5: 0.9938\n"
          ]
        }
      ]
    }
  ]
}